{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vision-Transformer-Cifar10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO5yDOzTPzs7atRq+Gf+Wsk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d944c8655e2d4fdcbf80ba181d56d0af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8f7b2bead644461fad901f8aad940472",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_05f7b43ccf0d48d9a1a24d4fc59fefe0",
              "IPY_MODEL_f9edf17694c24c4f86a1186401471be9"
            ]
          }
        },
        "8f7b2bead644461fad901f8aad940472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05f7b43ccf0d48d9a1a24d4fc59fefe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_34bcc35ee2aa4d8b8f149983c6b29470",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ea3967216974272a8348c9659681572"
          }
        },
        "f9edf17694c24c4f86a1186401471be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_256e07679781455d83ce8f0be542b583",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 51638935.01it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b91c3707df8482f8b26684f6dcc6621"
          }
        },
        "34bcc35ee2aa4d8b8f149983c6b29470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ea3967216974272a8348c9659681572": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "256e07679781455d83ce8f0be542b583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b91c3707df8482f8b26684f6dcc6621": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/Vision-Transformer-Study/blob/main/Vision_Transformer_Cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MANKFlOvF5V"
      },
      "source": [
        "\r\n",
        "Vision-Transformers with CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqFb9BsovDHZ"
      },
      "source": [
        "# -*- coding: utf-8 -*-\r\n",
        "'''\r\n",
        "Train CIFAR10 with PyTorch and Vision Transformers!\r\n",
        "written by @kentaroy47, @arutema47\r\n",
        "'''\r\n",
        "\r\n",
        "from __future__ import print_function\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.backends.cudnn as cudnn\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "\r\n",
        "import os\r\n",
        "import argparse\r\n",
        "import pandas as pd\r\n",
        "import csv\r\n",
        "\r\n",
        "#from models import *\r\n",
        "#from models.vit import ViT\r\n",
        "#from utils import progress_bar"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyEsD1OKxm9Y",
        "outputId": "2c191642-56d1-4092-d587-ce59556a6548"
      },
      "source": [
        "!pip install einops\r\n",
        "!pip install odach"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.0\n",
            "Collecting odach\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/5c/2a7c44cf6c4ac4e933596ca44c068b509d67bf0abb610b779e864edf1880/odach-0.1.5.post2011180124-py3-none-any.whl\n",
            "Installing collected packages: odach\n",
            "Successfully installed odach-0.1.5.post2011180124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv-jxJqg3tl-"
      },
      "source": [
        "# -*- coding: utf-8 -*-\r\n",
        "\r\n",
        "'''Some helper functions for PyTorch, including:\r\n",
        "    - get_mean_and_std: calculate the mean and std value of dataset.\r\n",
        "    - msr_init: net parameter initialization.\r\n",
        "    - progress_bar: progress bar mimic xlua.progress.\r\n",
        "'''\r\n",
        "import os\r\n",
        "import sys\r\n",
        "import time\r\n",
        "import math\r\n",
        "\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.init as init\r\n",
        "\r\n",
        "\r\n",
        "def get_mean_and_std(dataset):\r\n",
        "    '''Compute the mean and std value of dataset.'''\r\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\r\n",
        "    mean = torch.zeros(3)\r\n",
        "    std = torch.zeros(3)\r\n",
        "    print('==> Computing mean and std..')\r\n",
        "    for inputs, targets in dataloader:\r\n",
        "        for i in range(3):\r\n",
        "            mean[i] += inputs[:,i,:,:].mean()\r\n",
        "            std[i] += inputs[:,i,:,:].std()\r\n",
        "    mean.div_(len(dataset))\r\n",
        "    std.div_(len(dataset))\r\n",
        "    return mean, std\r\n",
        "\r\n",
        "def init_params(net):\r\n",
        "    '''Init layer parameters.'''\r\n",
        "    for m in net.modules():\r\n",
        "        if isinstance(m, nn.Conv2d):\r\n",
        "            init.kaiming_normal(m.weight, mode='fan_out')\r\n",
        "            if m.bias:\r\n",
        "                init.constant(m.bias, 0)\r\n",
        "        elif isinstance(m, nn.BatchNorm2d):\r\n",
        "            init.constant(m.weight, 1)\r\n",
        "            init.constant(m.bias, 0)\r\n",
        "        elif isinstance(m, nn.Linear):\r\n",
        "            init.normal(m.weight, std=1e-3)\r\n",
        "            if m.bias:\r\n",
        "                init.constant(m.bias, 0)\r\n",
        "\r\n",
        "\r\n",
        "try:\r\n",
        "\t_, term_width = os.popen('stty size', 'r').read().split()\r\n",
        "except:\r\n",
        "\tterm_width = 80\r\n",
        "term_width = int(term_width)\r\n",
        "\r\n",
        "TOTAL_BAR_LENGTH = 65.\r\n",
        "last_time = time.time()\r\n",
        "begin_time = last_time\r\n",
        "def progress_bar(current, total, msg=None):\r\n",
        "    global last_time, begin_time\r\n",
        "    if current == 0:\r\n",
        "        begin_time = time.time()  # Reset for new bar.\r\n",
        "\r\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\r\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\r\n",
        "\r\n",
        "    sys.stdout.write(' [')\r\n",
        "    for i in range(cur_len):\r\n",
        "        sys.stdout.write('=')\r\n",
        "    sys.stdout.write('>')\r\n",
        "    for i in range(rest_len):\r\n",
        "        sys.stdout.write('.')\r\n",
        "    sys.stdout.write(']')\r\n",
        "\r\n",
        "    cur_time = time.time()\r\n",
        "    step_time = cur_time - last_time\r\n",
        "    last_time = cur_time\r\n",
        "    tot_time = cur_time - begin_time\r\n",
        "\r\n",
        "    L = []\r\n",
        "    L.append('  Step: %s' % format_time(step_time))\r\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\r\n",
        "    if msg:\r\n",
        "        L.append(' | ' + msg)\r\n",
        "\r\n",
        "    msg = ''.join(L)\r\n",
        "    sys.stdout.write(msg)\r\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\r\n",
        "        sys.stdout.write(' ')\r\n",
        "\r\n",
        "    # Go back to the center of the bar.\r\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\r\n",
        "        sys.stdout.write('\\b')\r\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\r\n",
        "\r\n",
        "    if current < total-1:\r\n",
        "        sys.stdout.write('\\r')\r\n",
        "    else:\r\n",
        "        sys.stdout.write('\\n')\r\n",
        "    sys.stdout.flush()\r\n",
        "\r\n",
        "def format_time(seconds):\r\n",
        "    days = int(seconds / 3600/24)\r\n",
        "    seconds = seconds - days*3600*24\r\n",
        "    hours = int(seconds / 3600)\r\n",
        "    seconds = seconds - hours*3600\r\n",
        "    minutes = int(seconds / 60)\r\n",
        "    seconds = seconds - minutes*60\r\n",
        "    secondsf = int(seconds)\r\n",
        "    seconds = seconds - secondsf\r\n",
        "    millis = int(seconds*1000)\r\n",
        "\r\n",
        "    f = ''\r\n",
        "    i = 1\r\n",
        "    if days > 0:\r\n",
        "        f += str(days) + 'D'\r\n",
        "        i += 1\r\n",
        "    if hours > 0 and i <= 2:\r\n",
        "        f += str(hours) + 'h'\r\n",
        "        i += 1\r\n",
        "    if minutes > 0 and i <= 2:\r\n",
        "        f += str(minutes) + 'm'\r\n",
        "        i += 1\r\n",
        "    if secondsf > 0 and i <= 2:\r\n",
        "        f += str(secondsf) + 's'\r\n",
        "        i += 1\r\n",
        "    if millis > 0 and i <= 2:\r\n",
        "        f += str(millis) + 'ms'\r\n",
        "        i += 1\r\n",
        "    if f == '':\r\n",
        "        f = '0ms'\r\n",
        "    return f"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LJvRhYBxhFP"
      },
      "source": [
        "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit_pytorch.py\r\n",
        "\r\n",
        "from einops import rearrange\r\n",
        "\r\n",
        "\r\n",
        "MIN_NUM_PATCHES = 16\r\n",
        "\r\n",
        "class Residual(nn.Module):\r\n",
        "    def __init__(self, fn):\r\n",
        "        super().__init__()\r\n",
        "        self.fn = fn\r\n",
        "    def forward(self, x, **kwargs):\r\n",
        "        return self.fn(x, **kwargs) + x\r\n",
        "\r\n",
        "class PreNorm(nn.Module):\r\n",
        "    def __init__(self, dim, fn):\r\n",
        "        super().__init__()\r\n",
        "        self.norm = nn.LayerNorm(dim)\r\n",
        "        self.fn = fn\r\n",
        "    def forward(self, x, **kwargs):\r\n",
        "        return self.fn(self.norm(x), **kwargs)\r\n",
        "\r\n",
        "class FeedForward(nn.Module):\r\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\r\n",
        "        super().__init__()\r\n",
        "        self.net = nn.Sequential(\r\n",
        "            nn.Linear(dim, hidden_dim),\r\n",
        "            nn.GELU(),\r\n",
        "            nn.Dropout(dropout),\r\n",
        "            nn.Linear(hidden_dim, dim),\r\n",
        "            nn.Dropout(dropout)\r\n",
        "        )\r\n",
        "    def forward(self, x):\r\n",
        "        return self.net(x)\r\n",
        "\r\n",
        "class Attention(nn.Module):\r\n",
        "    def __init__(self, dim, heads = 8, dropout = 0.):\r\n",
        "        super().__init__()\r\n",
        "        self.heads = heads\r\n",
        "        self.scale = dim ** -0.5\r\n",
        "\r\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias = False)\r\n",
        "        self.to_out = nn.Sequential(\r\n",
        "            nn.Linear(dim, dim),\r\n",
        "            nn.Dropout(dropout)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x, mask = None):\r\n",
        "        b, n, _, h = *x.shape, self.heads\r\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\r\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\r\n",
        "\r\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\r\n",
        "\r\n",
        "        if mask is not None:\r\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\r\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\r\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\r\n",
        "            dots.masked_fill_(~mask, float('-inf'))\r\n",
        "            del mask\r\n",
        "\r\n",
        "        attn = dots.softmax(dim=-1)\r\n",
        "\r\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\r\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\r\n",
        "        out =  self.to_out(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "class Transformer(nn.Module):\r\n",
        "    def __init__(self, dim, depth, heads, mlp_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "        self.layers = nn.ModuleList([])\r\n",
        "        for _ in range(depth):\r\n",
        "            self.layers.append(nn.ModuleList([\r\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads, dropout = dropout))),\r\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)))\r\n",
        "            ]))\r\n",
        "    def forward(self, x, mask = None):\r\n",
        "        for attn, ff in self.layers:\r\n",
        "            x = attn(x, mask = mask)\r\n",
        "            x = ff(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "class ViT(nn.Module):\r\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dropout = 0., emb_dropout = 0.):\r\n",
        "        super().__init__()\r\n",
        "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\r\n",
        "        num_patches = (image_size // patch_size) ** 2\r\n",
        "        patch_dim = channels * patch_size ** 2\r\n",
        "        assert num_patches > MIN_NUM_PATCHES, f'your number of patches ({num_patches}) is way too small for attention to be effective. try decreasing your patch size'\r\n",
        "\r\n",
        "        self.patch_size = patch_size\r\n",
        "\r\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\r\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\r\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\r\n",
        "        self.dropout = nn.Dropout(emb_dropout)\r\n",
        "\r\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout)\r\n",
        "\r\n",
        "        self.to_cls_token = nn.Identity()\r\n",
        "\r\n",
        "        self.mlp_head = nn.Sequential(\r\n",
        "            nn.LayerNorm(dim),\r\n",
        "            nn.Linear(dim, mlp_dim),\r\n",
        "            nn.GELU(),\r\n",
        "            nn.Dropout(dropout),\r\n",
        "            nn.Linear(mlp_dim, num_classes)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, img, mask = None):\r\n",
        "        p = self.patch_size\r\n",
        "\r\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\r\n",
        "        x = self.patch_to_embedding(x)\r\n",
        "        b, n, _ = x.shape\r\n",
        "\r\n",
        "        cls_tokens = self.cls_token.expand(b, -1, -1)\r\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\r\n",
        "        x += self.pos_embedding[:, :(n + 1)]\r\n",
        "        x = self.dropout(x)\r\n",
        "\r\n",
        "        x = self.transformer(x, mask)\r\n",
        "\r\n",
        "        x = self.to_cls_token(x[:, 0])\r\n",
        "        return self.mlp_head(x)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw6OuNQT0Ihc",
        "outputId": "6bb7fa6b-4059-403b-e3a0-079c67e63b5b"
      },
      "source": [
        "!pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n",
            "  Cloning https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to /tmp/pip-req-build-prno4vzs\n",
            "  Running command git clone -q https://github.com/ildoonet/pytorch-gradual-warmup-lr.git /tmp/pip-req-build-prno4vzs\n",
            "Building wheels for collected packages: warmup-scheduler\n",
            "  Building wheel for warmup-scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warmup-scheduler: filename=warmup_scheduler-0.3.2-cp36-none-any.whl size=3881 sha256=60fbf309c3b9271f77f4183d334c051054fe7ab48ad6c4355bfdd8f40c78d00a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-stl6urfk/wheels/b7/24/83/d30234cc013cff538805b14df916e79091f7cf9ee2c5bf3a64\n",
            "Successfully built warmup-scheduler\n",
            "Installing collected packages: warmup-scheduler\n",
            "Successfully installed warmup-scheduler-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8XGCiw4vPjc"
      },
      "source": [
        "lr=1e-4 # learning rate. for resnets.. 1e-3, Vit..1e-4?\r\n",
        "opt=\"adam\"\r\n",
        "resume= 0  # resume from checkpoin\r\n",
        "aug=!      # add image augumentations\r\n",
        "mixup=1    # add mixup augumentations\r\n",
        "#net=vit\r\n",
        "bs=64\r\n",
        "n_epochs=100\r\n",
        "patch=4\r\n",
        "cos=1      # Train with cosine annealing scheduling\r\n",
        "\r\n",
        "if cos:\r\n",
        "    from warmup_scheduler import GradualWarmupScheduler\r\n",
        "if aug:\r\n",
        "    import albumentations"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119,
          "referenced_widgets": [
            "d944c8655e2d4fdcbf80ba181d56d0af",
            "8f7b2bead644461fad901f8aad940472",
            "05f7b43ccf0d48d9a1a24d4fc59fefe0",
            "f9edf17694c24c4f86a1186401471be9",
            "34bcc35ee2aa4d8b8f149983c6b29470",
            "3ea3967216974272a8348c9659681572",
            "256e07679781455d83ce8f0be542b583",
            "0b91c3707df8482f8b26684f6dcc6621"
          ]
        },
        "id": "aiORS-0CzlEu",
        "outputId": "9aa6c7f3-3d00-4f3f-a328-b307d55f93f1"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "best_acc = 0  # best test accuracy\r\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\r\n",
        "\r\n",
        "# Data\r\n",
        "print('==> Preparing data..')\r\n",
        "transform_train = transforms.Compose([\r\n",
        "    transforms.RandomCrop(32, padding=4),\r\n",
        "    transforms.RandomHorizontalFlip(),\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\r\n",
        "])\r\n",
        "\r\n",
        "transform_test = transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\r\n",
        "])\r\n",
        "\r\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\r\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=8)\r\n",
        "\r\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\r\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=8)\r\n",
        "\r\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d944c8655e2d4fdcbf80ba181d56d0af",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLJUzJ2Y11nb"
      },
      "source": [
        "# ViT for cifar10\r\n",
        "net = ViT(\r\n",
        "    image_size = 32,\r\n",
        "    patch_size = patch,\r\n",
        "    num_classes = 10,\r\n",
        "    dim = 512,\r\n",
        "    depth = 6,\r\n",
        "    heads = 8,\r\n",
        "    mlp_dim = 512,\r\n",
        "    dropout = 0.1,\r\n",
        "    emb_dropout = 0.1\r\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZw-kjtc2Pog",
        "outputId": "66a86c77-c231-4b1d-db9d-931cf40b97f6"
      },
      "source": [
        "ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoint\u001b[0m/  \u001b[01;34mdata\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip3jcVzM18nw"
      },
      "source": [
        "net = net.to(device)\r\n",
        "if device == 'cuda':\r\n",
        "    net = torch.nn.DataParallel(net) # make parallel\r\n",
        "    cudnn.benchmark = True\r\n",
        "\r\n",
        "if resume:\r\n",
        "    # Load checkpoint.\r\n",
        "    print('==> Resuming from checkpoint..')\r\n",
        "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\r\n",
        "    checkpoint = torch.load('./checkpoint/{}-ckpt.t7'.format(\"vit\"))\r\n",
        "    net.load_state_dict(checkpoint['net'])\r\n",
        "    best_acc = checkpoint['acc']\r\n",
        "    start_epoch = checkpoint['epoch']\r\n",
        "\r\n",
        "# Loss is CE\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "# reduce LR on Plateau\r\n",
        "if opt == \"adam\":\r\n",
        "    optimizer = optim.Adam(net.parameters(), lr)\r\n",
        "elif opt == \"sgd\":\r\n",
        "    optimizer = optim.SGD(net.parameters(), lr)    \r\n",
        "if not cos:\r\n",
        "    from torch.optim import lr_scheduler\r\n",
        "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True, min_lr=1e-3*1e-5, factor=0.1)\r\n",
        "else:\r\n",
        "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs-1)\r\n",
        "    scheduler = GradualWarmupScheduler(optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\r\n",
        "    "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVQqGdlH2mjh"
      },
      "source": [
        "##### Training\r\n",
        "def train(epoch):\r\n",
        "    print('\\nEpoch: %d' % epoch)\r\n",
        "    net.train()\r\n",
        "    train_loss = 0\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\r\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        outputs = net(inputs)\r\n",
        "        loss = criterion(outputs, targets)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        train_loss += loss.item()\r\n",
        "        _, predicted = outputs.max(1)\r\n",
        "        total += targets.size(0)\r\n",
        "        correct += predicted.eq(targets).sum().item()\r\n",
        "\r\n",
        "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\r\n",
        "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\r\n",
        "    return train_loss/(batch_idx+1)\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "LE_ri1Wv2157",
        "outputId": "a661c6d5-c3f0-4cfc-cda2-5248b4a322c8"
      },
      "source": [
        "##### Validation\r\n",
        "import time\r\n",
        "def test(epoch):\r\n",
        "    global best_acc\r\n",
        "    net.eval()\r\n",
        "    test_loss = 0\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\r\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\r\n",
        "            outputs = net(inputs)\r\n",
        "            loss = criterion(outputs, targets)\r\n",
        "\r\n",
        "            test_loss += loss.item()\r\n",
        "            _, predicted = outputs.max(1)\r\n",
        "            total += targets.size(0)\r\n",
        "            correct += predicted.eq(targets).sum().item()\r\n",
        "\r\n",
        "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\r\n",
        "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\r\n",
        "    \r\n",
        "    # Update scheduler\r\n",
        "    if not cos:\r\n",
        "        scheduler.step(test_loss)\r\n",
        "    \r\n",
        "    # Save checkpoint.\r\n",
        "    acc = 100.*correct/total\r\n",
        "    if acc > best_acc:\r\n",
        "        print('Saving..')\r\n",
        "        state = {\r\n",
        "            'net': net.state_dict(),\r\n",
        "            'acc': acc,\r\n",
        "            'epoch': epoch,\r\n",
        "        }\r\n",
        "        if not os.path.isdir('checkpoint'):\r\n",
        "            os.mkdir('checkpoint')\r\n",
        "        torch.save(state, './checkpoint/'+'vit'+'-{}-ckpt.t7'.format(args.patch))\r\n",
        "        best_acc = acc\r\n",
        "    \r\n",
        "    os.makedirs(\"log\", exist_ok=True)\r\n",
        "    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, val loss: {test_loss:.5f}, acc: {(acc):.5f}'\r\n",
        "    print(content)\r\n",
        "    with open(f'log/log_vit.txt', 'a') as appender:\r\n",
        "        appender.write(content + \"\\n\")\r\n",
        "    return test_loss, acc\r\n",
        "\r\n",
        "list_loss = []\r\n",
        "list_acc = []\r\n",
        "for epoch in range(start_epoch, n_epochs):\r\n",
        "    trainloss = train(epoch)\r\n",
        "    val_loss, acc = test(epoch)\r\n",
        "    \r\n",
        "    if cos:\r\n",
        "        scheduler.step(epoch-1)\r\n",
        "    \r\n",
        "    list_loss.append(val_loss)\r\n",
        "    list_acc.append(acc)\r\n",
        "    \r\n",
        "    # write as csv for analysis\r\n",
        "    with open(f'log/log_vit.csv', 'w') as f:\r\n",
        "        writer = csv.writer(f, lineterminator='\\n')\r\n",
        "        writer.writerow(list_loss) \r\n",
        "        writer.writerow(list_acc) \r\n",
        "    print(list_loss)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            " [================================================================>]  Step: 44ms | Tot: 1m28s | Loss: 1.787 | Acc: 34.142% (17071/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 782/782 \n",
            " [================================================================>]  Step: 58ms | Tot: 6s268ms | Loss: 1.507 | Acc: 45.320% (4532/10000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 100/100 \n",
            "Saving..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2137a3c11975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mtrainloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-2137a3c11975>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./checkpoint/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'vit'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-{}-ckpt.t7'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    }
  ]
}