{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisionTransformer_MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFEOe7rm6kt7JmfEwKelnh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/Vision-Transformer-Study/blob/main/VisionTransformer_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qHuKzHs10J0"
      },
      "source": [
        "# Vision Transformer with Simple MNIST dataset!\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "최근 주목 받고 있는 Vision Transformer를 효과적으로 이해하기 위해서 Simple 이미지 데이터셋인 MNIST를 이용하여 Vision Transformer의 동작을 살펴봄.\r\n",
        "\r\n",
        "2021-02-04"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwzoajvCsu2H"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "# MNIST data - raining set / test set. Normalize\r\n",
        "\r\n",
        "torch.manual_seed(42)\r\n",
        "\r\n",
        "# MNIST 이미지 데이터 셋 다운로드\r\n",
        "DOWNLOAD_PATH = '/data/mnist'\r\n",
        "\r\n",
        "# 학습시 배치 사이즈 \r\n",
        "BATCH_SIZE_TRAIN = 100\r\n",
        "# 검증시 배치 사이즈\r\n",
        "BATCH_SIZE_TEST = 1000\r\n",
        "\r\n",
        "# MNIST 데이터셋은 \"28x28\" 사이즈의 손글씨 데이터셋\r\n",
        "#\r\n",
        "\r\n",
        "transform_mnist = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\r\n",
        "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))])\r\n",
        "\r\n",
        "train_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=True, download=True, transform=transform_mnist)\r\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\r\n",
        "\r\n",
        "test_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=False, download=True, transform=transform_mnist)\r\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE_TEST, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMB__P1wtQuA",
        "outputId": "70c81e08-4e63-4265-8719-9a699550e4d6"
      },
      "source": [
        "# [einops] stands for Einstein-Inspired Notation for operations\r\n",
        "# 텐서 연산을 보다 효과적으로 구성하기 위한 패키지\r\n",
        "!pip install einops"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.6/dist-packages (0.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9yCr9OwrSSe"
      },
      "source": [
        "## Vision Transformer Overal Model View\r\n",
        "\r\n",
        "The vision transformer does not use any CNN style filters for detecting any features from images. It just uses *Self-Attention* mechanism with queries, keys, values derived from input patches (in sequence).\r\n",
        "\r\n",
        "\r\n",
        "This is the most interesting point of Vision Transformer !\r\n",
        "\r\n",
        "![Vit](https://github.com/jeonggunlee/Vision-Transformer-Study/blob/main/image1.gif?raw=1)\r\n",
        "\r\n",
        "Ref: https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTdNQEAaopU_"
      },
      "source": [
        "It is good to understand einsum package.\r\n",
        "\r\n",
        "Ref: https://theaisummer.com/einsum-attention/?fbclid=IwAR0uHuLWF4XJTAHpbHd6vOtzsqcsVAVtUHUvJJhIi7jDV_hc8RcPKIpGXI4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLb_DHqss52c"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torch import nn\r\n",
        "from einops import rearrange  # 이미지 구조 변경에 매우 용이한 유틸리티\r\n",
        "\r\n",
        "# Vision Transformer (ViT)의 오리지널 소스 코드\r\n",
        "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit_pytorch.py\r\n",
        "\r\n",
        "class Residual(nn.Module):\r\n",
        "    def __init__(self, fn):\r\n",
        "        super().__init__()\r\n",
        "        self.fn = fn\r\n",
        "\r\n",
        "    def forward(self, x, **kwargs):\r\n",
        "        return self.fn(x, **kwargs) + x\r\n",
        "\r\n",
        "class PreNorm(nn.Module):\r\n",
        "    def __init__(self, dim, fn):\r\n",
        "        super().__init__()\r\n",
        "        self.norm = nn.LayerNorm(dim)\r\n",
        "        self.fn = fn\r\n",
        "\r\n",
        "    def forward(self, x, **kwargs):\r\n",
        "        return self.fn(self.norm(x), **kwargs)\r\n",
        "\r\n",
        "class FeedForward(nn.Module):\r\n",
        "    def __init__(self, dim, hidden_dim):\r\n",
        "        super().__init__()\r\n",
        "        self.net = nn.Sequential(\r\n",
        "            nn.Linear(dim, hidden_dim),\r\n",
        "            nn.GELU(),\r\n",
        "            nn.Linear(hidden_dim, dim)  # Note: Activation 없음\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.net(x)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ynbI2QLnup"
      },
      "source": [
        "\r\n",
        "## Multi-head Attention Module\r\n",
        " With Query, Key, Value !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW2otu7kLSae"
      },
      "source": [
        "class Attention(nn.Module):        # Attention 모듈\r\n",
        "    def __init__(self, dim, heads=8):\r\n",
        "        super().__init__()\r\n",
        "        self.heads = heads         # head의 수\r\n",
        "        self.scale = dim ** -0.5   # scaled dot-product에 사용될 scale factor: Sqrt(dim)\r\n",
        "\r\n",
        "        ## Query, Key, Value에 대해서 prject을 수행\r\n",
        "        ## dim --> dim * 3 => projected Q, projected K, project V\r\n",
        "        ## Self attention 구성을 위해서 하나의 X 값을 입력 받아, Qx, Kx, Vx를 구성한다\r\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\r\n",
        "        self.to_out = nn.Linear(dim, dim)\r\n",
        "\r\n",
        "    def forward(self, x, mask = None):\r\n",
        "        b, n, _, h = *x.shape, self.heads\r\n",
        "        # print(\"X shape\", x.shape)\r\n",
        "        # X shape torch.Size([100, 17, 64]) : 패치(16) + class(1) 등 총 17개의 64차원으로 embeding된 데이터가 들어옮\r\n",
        "\r\n",
        "        qkv = self.to_qkv(x)                 # ## Self attention 구성을 위해서 하나의 X 값을 입력 받아, Qx, Kx, Vx를 구성한다\r\n",
        "        # print(\"qkv shape\", qkv.shape)\r\n",
        "        # qkv shape torch.Size([100, 17, 192]) : 64 x 3 -> 192\r\n",
        "        # 192 = qkv * h * d = qkv * 8 * d ==> qkv * d = 24\r\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)  # b : # batches\r\n",
        "                                                                              # n : # of sequence = # of patches + 1\r\n",
        "                                                                              # qkv: query, key, value\r\n",
        "                                                                              # h : # of heads\r\n",
        "                                                                              # d : # of dimension\r\n",
        "        #q shape torch.Size([100, 8, 17, 8])  batch, head, seqlength, dimension\r\n",
        "        #k shape torch.Size([100, 8, 17, 8])\r\n",
        "        #v shape torch.Size([100, 8, 17, 8])\r\n",
        "\r\n",
        "        # Scaled Dot Product 계산: Self-Attention !!!\r\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\r\n",
        "        # print(\"dots shape\", dots.shape)\r\n",
        "        # dots shape torch.Size([100, 8, 17, 17]) --> [17, 17] is an attention map\r\n",
        "\r\n",
        "        if mask is not None:\r\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\r\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\r\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\r\n",
        "            dots.masked_fill_(~mask, float('-inf'))\r\n",
        "            del mask\r\n",
        "\r\n",
        "        # Softmax를 통해서 Attention 계산\r\n",
        "        attn = dots.softmax(dim=-1)\r\n",
        "        # print(\"Attention shape\", attn.shape)\r\n",
        "        # Attention shape torch.Size([100, 8, 17, 17])\r\n",
        "\r\n",
        "        # Attention * Value for each head for each batch\r\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\r\n",
        "        # print(\"Value * Attention shape\", out.shape)\r\n",
        "        # Value * Attention shape torch.Size([100, 8, 17, 8])\r\n",
        "\r\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\r\n",
        "        \r\n",
        "        out =  self.to_out(out)  # nn.Linear(dim, dim)\r\n",
        "        return out\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdRSCgaxu32o"
      },
      "source": [
        "By printing out ```\"attn\"```, you can check how attention maps are derived from input patch sequences.\r\n",
        "\r\n",
        "What does the attention betweeen image patches actually mean ? It maybe a very fundamental question for understanding the transformer's ability of classifying image classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eD6jmZuMALE"
      },
      "source": [
        "## Transformer (Encoder Part Only)= Multi-head Attention + FeedForward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYzizu4BLXxC"
      },
      "source": [
        "class Transformer(nn.Module):\r\n",
        "    def __init__(self, dim, depth, heads, mlp_dim):\r\n",
        "        super().__init__()\r\n",
        "        self.layers = nn.ModuleList([])\r\n",
        "        for _ in range(depth):\r\n",
        "            self.layers.append(nn.ModuleList([\r\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\r\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\r\n",
        "            ]))\r\n",
        "\r\n",
        "    def forward(self, x, mask=None):\r\n",
        "        for attn, ff in self.layers:\r\n",
        "            x = attn(x, mask=mask)\r\n",
        "            x = ff(x)\r\n",
        "        return x\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62a9xTXpMKRL"
      },
      "source": [
        "## Visual Transformer\r\n",
        "\r\n",
        "![Transformer](https://nlpinkorean.github.io/images/transformer/transformer_resideual_layer_norm_2.png)\r\n",
        "\r\n",
        "Ref: https://nlpinkorean.github.io/illustrated-transformer/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3TjRyu4LazJ"
      },
      "source": [
        "class ViT(nn.Module):\r\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\r\n",
        "        super().__init__()\r\n",
        "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\r\n",
        "        num_patches = (image_size // patch_size) ** 2\r\n",
        "        # patch의 수 x축 패치 수와 y축 패치 수를 곱하고 다시 채널수를 곱함. ==> Sequence Length (n) = 17\r\n",
        "        # MNIST의 경우 채널이 하나이기 때문에 channels = 1\r\n",
        "        patch_dim = channels * patch_size ** 2  # 7*7 = 49. 추후 임베딩을 통해서 64로 변경\r\n",
        "\r\n",
        "        # 패치 하나의 길이 in pixel\r\n",
        "        self.patch_size = patch_size\r\n",
        "\r\n",
        "        # 위치 임베딩을 위한 학습 파라미터 생성\r\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\r\n",
        "\r\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)  # 49 --> 64\r\n",
        "\r\n",
        "        # 클래스 토큰 파라미터 생성\r\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\r\n",
        "\r\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\r\n",
        "\r\n",
        "        self.to_cls_token = nn.Identity()\r\n",
        "\r\n",
        "        self.mlp_head = nn.Sequential(\r\n",
        "            nn.Linear(dim, mlp_dim),\r\n",
        "            nn.GELU(),\r\n",
        "            nn.Linear(mlp_dim, num_classes)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, img, mask=None):\r\n",
        "        p = self.patch_size\r\n",
        "\r\n",
        "        # rearrange\r\n",
        "        # batch channel (head p1) (w p2) -> batch (h w) (p1 p2 c)\r\n",
        "        # h : # of patches in a vertical view\r\n",
        "        # w : # of patches in a horizontal view\r\n",
        "        # In MNIST: h=4, w=4, p1=7, p2=7, c=1\r\n",
        "        #print(\"Input X\", img.shape)\r\n",
        "        # Input X torch.Size([100, 1, 28, 28])\r\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\r\n",
        "        #print(\"Rearrange X\", x.shape)\r\n",
        "        # Rearrange X torch.Size([100, 16, 49])\r\n",
        "        x = self.patch_to_embedding(x)     # (h w) --> dim vector\r\n",
        "        # b (h w) (p1 p2 c) --> b (h w) dim\r\n",
        "        #print(\"Embedding X\", x.shape)\r\n",
        "        # Embedding X torch.Size([100, 16, 64])\r\n",
        "\r\n",
        "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\r\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\r\n",
        "        x += self.pos_embedding\r\n",
        "        #print(\"Pos Embedding X\", x.shape)\r\n",
        "        #Pos Embedding X torch.Size([100, 17, 64])\r\n",
        "        x = self.transformer(x, mask)\r\n",
        "        #print(\"After Transformer\", x.shape)\r\n",
        "        #After Transformer torch.Size([100, 17, 64])\r\n",
        "        x = self.to_cls_token(x[:, 0])\r\n",
        "        #print(\"cls token X\", x.shape)\r\n",
        "        #cls token X torch.Size([100, 64])\r\n",
        "        return self.mlp_head(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fg1VVtfMQCl"
      },
      "source": [
        "## Train and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDhzDayptVsi"
      },
      "source": [
        "# training / evaluate function\r\n",
        "\r\n",
        "def train_epoch(model, optimizer, data_loader, loss_history):\r\n",
        "    total_samples = len(data_loader.dataset)\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    # data ~ torch.Size([100, 1, 28, 28])\r\n",
        "    # batch size = 100\r\n",
        "    for i, (data, target) in enumerate(data_loader):\r\n",
        "        optimizer.zero_grad()\r\n",
        "        # Model output shape: [100, 10]\r\n",
        "        output = F.log_softmax(model(data), dim=1)             \r\n",
        "        loss = F.nll_loss(output, target)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        if i % 100 == 0:\r\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\r\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\r\n",
        "                  '{:6.4f}'.format(loss.item()))\r\n",
        "            loss_history.append(loss.item())\r\n",
        "\r\n",
        "def evaluate(model, data_loader, loss_history):\r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    total_samples = len(data_loader.dataset)\r\n",
        "    correct_samples = 0\r\n",
        "    total_loss = 0\r\n",
        "    \r\n",
        "    # avg test loss / avg accuracy\r\n",
        "    with torch.no_grad():\r\n",
        "        for data, target in data_loader:\r\n",
        "            output = F.log_softmax(model(data), dim=1)\r\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\r\n",
        "            _, pred = torch.max(output, dim=1)\r\n",
        "            \r\n",
        "            total_loss += loss.item()\r\n",
        "            correct_samples += pred.eq(target).sum()\r\n",
        "\r\n",
        "    avg_loss = total_loss / total_samples\r\n",
        "    loss_history.append(avg_loss)\r\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\r\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\r\n",
        "          '{:5}'.format(total_samples) + ' (' +\r\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8vrs_x1ta-h",
        "outputId": "fbb67f74-07bb-4195-cb16-e9f02674e6e5"
      },
      "source": [
        "import time\r\n",
        "\r\n",
        "N_EPOCHS = 1  # 25 default. 1 just for debugging\r\n",
        "\r\n",
        "start_time = time.time()\r\n",
        "\r\n",
        "# Vision Transformer !\r\n",
        "model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1, dim=64, depth=6, heads=8, mlp_dim=128)\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\r\n",
        "\r\n",
        "train_loss_history, test_loss_history = [], []\r\n",
        "for epoch in range(1, N_EPOCHS + 1):\r\n",
        "    print('Epoch:', epoch)\r\n",
        "    train_epoch(model, optimizer, train_loader, train_loss_history)\r\n",
        "    evaluate(model, test_loader, test_loss_history)\r\n",
        "\r\n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "[    0/60000 (  0%)]  Loss: 2.3220\n",
            "[10000/60000 ( 17%)]  Loss: 0.5522\n",
            "[20000/60000 ( 33%)]  Loss: 0.3151\n",
            "[30000/60000 ( 50%)]  Loss: 0.3877\n",
            "[40000/60000 ( 67%)]  Loss: 0.0556\n",
            "[50000/60000 ( 83%)]  Loss: 0.2065\n",
            "\n",
            "Average test loss: 0.1384  Accuracy: 9558/10000 (95.58%)\n",
            "\n",
            "Execution time: 90.96 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}